{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "886edf72-8b5a-44df-9930-6c6e849e4142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# : Importing dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb29be2-0bba-480f-b25c-5e7a5f396d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Loading and clean events.csv\n",
    "events = pd.read_csv(\"events.csv\")\n",
    "events[\"timestamp\"] = pd.to_datetime(events[\"timestamp\"], unit=\"ms\")\n",
    "events[\"date\"] = events[\"timestamp\"].dt.date\n",
    "events[\"event_weight\"] = events[\"event\"].map({\"view\": 1, \"addtocart\": 2, \"transaction\": 3})\n",
    "events.dropna(subset=[\"event_weight\"], inplace=True)\n",
    "\n",
    "#  Aggregate daily activity\n",
    "daily_activity = events.groupby([\"visitorid\", \"date\"])[\"event_weight\"]\\\n",
    "    .sum().reset_index().rename(columns={\"event_weight\": \"activity_count\"})\n",
    "daily_activity[\"date\"] = pd.to_datetime(daily_activity[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5631fc4d-b59e-434f-96fe-44531a55e00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Pad dates per visitor\n",
    "all_dates = pd.date_range(daily_activity[\"date\"].min(), daily_activity[\"date\"].max())\n",
    "active_users = daily_activity.groupby(\"visitorid\")[\"activity_count\"].sum().sort_values(ascending=False).head(200).index.tolist()\n",
    "\n",
    "X_all = []\n",
    "y_all = []\n",
    "\n",
    "for visitor in active_users:\n",
    "    user_data = daily_activity[daily_activity[\"visitorid\"] == visitor]\n",
    "    padded = pd.DataFrame({\n",
    "        \"date\": all_dates,\n",
    "        \"activity_count\": 0\n",
    "    })\n",
    "    padded = padded.merge(user_data, on=\"date\", how=\"left\", suffixes=(\"\", \"_real\"))\n",
    "    padded[\"activity_count\"] = padded[\"activity_count_real\"].fillna(padded[\"activity_count\"])\n",
    "    padded.drop(columns=[\"activity_count_real\"], inplace=True)\n",
    "    padded[\"visitorid\"] = visitor\n",
    "    padded[\"dayofweek\"] = padded[\"date\"].dt.dayofweek\n",
    "    padded[\"rolling_mean\"] = padded[\"activity_count\"].rolling(3, min_periods=1).mean()\n",
    "    padded[\"rolling_std\"] = padded[\"activity_count\"].rolling(3, min_periods=1).std().fillna(0)\n",
    "    padded[\"prev_day\"] = padded[\"activity_count\"].shift(1).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d042ce4-3a3f-4f1f-99e3-200a507bfb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store raw and normalized activity\n",
    "padded[\"raw_count\"] = padded[\"activity_count\"]\n",
    "padded[\"activity_count\"] = (padded[\"activity_count\"] - padded[\"activity_count\"].mean()) / (padded[\"activity_count\"].std() + 1e-5)\n",
    "\n",
    "# Select features\n",
    "features = [\"activity_count\", \"rolling_mean\", \"rolling_std\", \"prev_day\"]\n",
    "values = padded[features].values\n",
    "\n",
    "# Define sequence builder\n",
    "def create_sequences(data, target, input_window=60, forecast_window=5):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - input_window - forecast_window + 1):\n",
    "        X.append(data[i:i+input_window])\n",
    "        y.append(target[i+input_window:i+input_window+forecast_window])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Prepare inputs\n",
    "raw_target = padded[\"raw_count\"].values\n",
    "X_cust, y_cust = create_sequences(values, raw_target)\n",
    "if len(X_cust) > 0:\n",
    "    X_all.append(X_cust)\n",
    "    y_all.append(y_cust)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31d75d2f-6373-47ca-8d92-ab4e3269e6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Concatenate and split\n",
    "X = np.concatenate(X_all)\n",
    "y = np.concatenate(y_all)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23675bb1-6c27-48ab-8bf6-c724a5cee126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sriks\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 1.9073 - val_loss: 1.2428 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 1.8658 - val_loss: 1.2319 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314ms/step - loss: 1.8537 - val_loss: 1.2310 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 1.8471 - val_loss: 1.2292 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 1.8460 - val_loss: 1.2253 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 1.8439 - val_loss: 1.2266 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 1.8435 - val_loss: 1.2244 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 1.8441 - val_loss: 1.2247 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 1.8445 - val_loss: 1.2222 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 1.8411 - val_loss: 1.2215 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - loss: 1.8398 - val_loss: 1.2225 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 1.8394 - val_loss: 1.2205 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 1.8394 - val_loss: 1.2200 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 1.8391 - val_loss: 1.2204 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 1.8383 - val_loss: 1.2194 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 1.8385 - val_loss: 1.2176 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 1.8379 - val_loss: 1.2187 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 1.8370 - val_loss: 1.2181 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - loss: 1.8367 - val_loss: 1.2167 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 1.8355 - val_loss: 1.2167 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 1.8359 - val_loss: 1.2164 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 1.8349 - val_loss: 1.2158 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - loss: 1.8345 - val_loss: 1.2156 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 1.8354 - val_loss: 1.2155 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 1.8346 - val_loss: 1.2155 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - loss: 1.8344 - val_loss: 1.2156 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 1.8344 - val_loss: 1.2158 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - loss: 1.8346 - val_loss: 1.2156 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 1.8341 - val_loss: 1.2153 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 1.8343 - val_loss: 1.2150 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 1.8341 - val_loss: 1.2151 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 1.8333 - val_loss: 1.2158 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - loss: 1.8339 - val_loss: 1.2155 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 1.8334 - val_loss: 1.2159 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 1.8338 - val_loss: 1.2154 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - loss: 1.8337 - val_loss: 1.2158 - learning_rate: 5.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 1.8337 - val_loss: 1.2153 - learning_rate: 5.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 1.8339 - val_loss: 1.2150 - learning_rate: 5.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 1.8332 - val_loss: 1.2150 - learning_rate: 5.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 1.8331 - val_loss: 1.2150 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "#  Model\n",
    "model = Sequential([\n",
    "    LSTM(128, return_sequences=True, input_shape=(X.shape[1], X.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(y.shape[1])\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "]\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                    epochs=100, batch_size=64, callbacks=callbacks, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "134e65d2-47e3-4f1c-a121-f512ca7e3382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total customers: 1407580 split into 14076 batches of 100\n"
     ]
    }
   ],
   "source": [
    "# spliting into 100 customers chunks\n",
    "grouped = full_activity.groupby(\"customer_id\")[\"activity_count\"]\n",
    "all_customers = list(grouped.groups.keys())\n",
    "\n",
    "# Split into chunks of 100\n",
    "batch_size = 100\n",
    "chunks = [all_customers[i:i+batch_size] for i in range(0, len(all_customers), batch_size)]\n",
    "\n",
    "print(f\" Total customers: {len(all_customers)} split into {len(chunks)} batches of {batch_size}\")\n",
    "\n",
    "#groups the data by customer_id and then splits the list of customers into chunks of 100 customers.\n",
    "#It uses these chunks to process batches of customer data, which can be useful for training machine learning models in smaller, more manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea91467c-6576-454a-bcd6-5689054041fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 2910 batch files. Merging now...\n",
      " All forecast batches merged into forecast_per_customer.csv\n",
      " Total rows: 2037000\n",
      " Preview:\n",
      "   customer_id     day  predicted_activity\n",
      "0            0  Day +1                0.02\n",
      "1            0  Day +2                0.01\n",
      "2            0  Day +3                0.00\n",
      "3            0  Day +4                0.01\n",
      "4            0  Day +5                0.01\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob # helps in finding file when we have multiple files with a similar name\n",
    "\n",
    "# Grab all batch forecast files\n",
    "csv_files = sorted(glob.glob(\"forecast_batch_*.csv\"))\n",
    "\n",
    "print(f\" Found {len(csv_files)} batch files. Merging now...\")\n",
    "\n",
    "# Read and merge\n",
    "merged_df = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
    "\n",
    "# Save final merged file\n",
    "merged_df.to_csv(\"forecast_per_customer.csv\", index=False)\n",
    "\n",
    "print(\" All forecast batches merged into forecast_per_customer.csv\")\n",
    "print(f\" Total rows: {len(merged_df)}\")\n",
    "print(\" Preview:\")\n",
    "print(merged_df.head())\n",
    "#collects all batch files, reads and combines their data\n",
    "# and then saves the merged data into a new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06aae5b6-68e5-411d-a904-5904968abe8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Preview of Columns: ['customer_id', 'day', 'predicted_activity']\n",
      "Sample Rows:\n",
      "   customer_id     day  predicted_activity\n",
      "0            0  Day +1                0.02\n",
      "1            0  Day +2                0.01\n",
      "2            0  Day +3                0.00\n",
      "3            0  Day +4                0.01\n",
      "4            0  Day +5                0.01\n",
      "\n",
      " Null Values:\n",
      "customer_id           0\n",
      "day                   0\n",
      "predicted_activity    0\n",
      "dtype: int64\n",
      "\n",
      " Duplicate Rows: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load just a portion of the file to check structure\n",
    "df = pd.read_csv(\"forecast_per_customer.csv\", nrows=1000)\n",
    "\n",
    "print(\" Preview of Columns:\", df.columns.tolist())\n",
    "print(\"Sample Rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n Null Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\n Duplicate Rows:\", df.duplicated().sum())\n",
    "#loads the first 1000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f60f133-885b-4db4-bbca-998512a24c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Personalized Product Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a02668dd-3e39-4055-87b9-0fd64fb6a62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of weighted interaction data:\n",
      "       timestamp  customer_id action  item_id  weight\n",
      "0  1433221332117       257597   view   355908       1\n",
      "1  1433224214164       992329   view   248676       1\n",
      "2  1433221999827       111016   view   318965       1\n",
      "3  1433221955914       483717   view   253185       1\n",
      "4  1433221337106       951259   view   367447       1\n"
     ]
    }
   ],
   "source": [
    " #  Load events and assign weights\n",
    "import pandas as pd\n",
    "\n",
    "events = pd.read_csv(\"events.csv\", usecols=[\"timestamp\", \"visitorid\", \"event\", \"itemid\"])\n",
    "events.columns = [\"timestamp\", \"customer_id\", \"action\", \"item_id\"]\n",
    "\n",
    "# Assign weights based on type of action\n",
    "weights = {\n",
    "    \"view\": 1,\n",
    "    \"addtocart\": 2,\n",
    "    \"transaction\": 3\n",
    "}\n",
    "events[\"weight\"] = events[\"action\"].map(weights)\n",
    "\n",
    "print(\"Sample of weighted interaction data:\")\n",
    "print(events.head())\n",
    "#assing a weight to each type of customer action view, add to cart, transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d3baeb6-1310-4523-b6ac-5aa44e1ef3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of user-item scores:\n",
      "   customer_id  item_id  weight\n",
      "0            0    67045       1\n",
      "1            0   285930       1\n",
      "2            0   357564       1\n",
      "3            1    72028       1\n",
      "6            2   325215       3\n"
     ]
    }
   ],
   "source": [
    "# Aggregate scores per customer-item\n",
    "user_item_scores = events.groupby([\"customer_id\", \"item_id\"])[\"weight\"].sum().reset_index()\n",
    "\n",
    "# Optional: sort by score for each customer\n",
    "user_item_scores = user_item_scores.sort_values(by=[\"customer_id\", \"weight\"], ascending=[True, False])\n",
    "\n",
    "print(\"Sample of user-item scores:\")\n",
    "print(user_item_scores.head())\n",
    "# the score of each customer item pair by summing the weight values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd8a49ad-5750-4036-96ea-8f597ba44470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sample of enriched recommendations:\n",
      "   customer_id  item_id  weight category_id\n",
      "0            0    67045       1         333\n",
      "1            0   285930       1         NaN\n",
      "2            0   357564       1         256\n",
      "3            1    72028       1         NaN\n",
      "4            2   325215       3         299\n"
     ]
    }
   ],
   "source": [
    "##  Load item_properties and extract category info\n",
    "item_props = pd.read_csv(\"item_properties_part1.csv\", usecols=[\"timestamp\", \"itemid\", \"property\", \"value\"])\n",
    "item_props = item_props[item_props[\"property\"] == \"categoryid\"]\n",
    "item_props = item_props.drop_duplicates(subset=[\"itemid\"])\n",
    "item_props = item_props.rename(columns={\"itemid\": \"item_id\", \"value\": \"category_id\"})\n",
    "\n",
    "# Merge with user-item scores\n",
    "recs = pd.merge(user_item_scores, item_props[[\"item_id\", \"category_id\"]], on=\"item_id\", how=\"left\")\n",
    "\n",
    "print(\" Sample of enriched recommendations:\")\n",
    "print(recs.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "253fd88c-549b-4e0e-9f4d-905ebbf34c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved recs_multiuser.csv with 2145179 rows.\n"
     ]
    }
   ],
   "source": [
    "# Renaming weight to score for clarity\n",
    "recs = recs.rename(columns={\"weight\": \"score\"})\n",
    "\n",
    "# Save the final recommendations\n",
    "recs.to_csv(\"recs_multiuser.csv\", index=False)\n",
    "print(\" Saved recs_multiuser.csv with\", len(recs), \"rows.\")\n",
    "# here we are renaming weight column to score for my clarity by the score it can make \n",
    "#better prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45892736-e984-4eeb-b799-2089d488eb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "###phase3: creating marketing strategy logic to add in dashboard\n",
    "import pandas as pd\n",
    "\n",
    "# Load the forecast data\n",
    "forecast_df = pd.read_csv(\"forecast_per_customer.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf1071ec-9aa6-4285-b7c4-5d60da0c3fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Low activity expected. Try offering a discount to re-engage this customer.\n"
     ]
    }
   ],
   "source": [
    "# Example selected user (you can replace this with user input later)\n",
    "\n",
    "selected_user = int(input(\"Enter customer ID: \"))\n",
    "\n",
    "# Filter forecast data for that customer\n",
    "customer_forecast = forecast_df[forecast_df[\"customer_id\"] == selected_user]\n",
    "\n",
    "# Generate a smart marketing suggestion\n",
    "if not customer_forecast.empty:\n",
    "    avg_activity = customer_forecast[\"predicted_activity\"].mean()\n",
    "\n",
    "    if avg_activity >= 2:\n",
    "        print(\"High predicted engagement. Offer a loyalty reward or bundle deal.\")\n",
    "    elif 1 <= avg_activity < 2:\n",
    "        print(\"Moderate activity. Consider sending a product reminder or wishlist notification.\")\n",
    "    else:\n",
    "        print(\"Low activity expected. Try offering a discount to re-engage this customer.\")\n",
    "else:\n",
    "    print(\"No forecast found for this customer.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0224e73-59ec-4310-a0ac-3962723acfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved forecast_per_customer_demo.csv and recs_multiuser_demo.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading full forecast and recommendations files\n",
    "forecast_full = pd.read_csv(\"forecast_per_customer.csv\")\n",
    "recs_full = pd.read_csv(\"recs_multiuser.csv\")\n",
    "\n",
    "# Step 1: Randomly sample 1000 unique customer IDs\n",
    "sampled_customers = forecast_full['customer_id'].drop_duplicates().sample(1000, random_state=42)\n",
    "\n",
    "# Step 2: Filter forecast data\n",
    "forecast_demo = forecast_full[forecast_full['customer_id'].isin(sampled_customers)]\n",
    "\n",
    "# Step 3: Filter recommendation data to same customers\n",
    "recs_demo = recs_full[recs_full['customer_id'].isin(sampled_customers)]\n",
    "\n",
    "# Step 4: Save demo versions for GitHub upload\n",
    "forecast_demo.to_csv(\"forecast_per_customer_demo.csv\", index=False)\n",
    "recs_demo.to_csv(\"recs_multiuser_demo.csv\", index=False)\n",
    "\n",
    "print(\" Saved forecast_per_customer_demo.csv and recs_multiuser_demo.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac0941d1-84a7-4d08-add2-baa3b9cd0e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sample Customer IDs: [1009078, 1090330, 1090334, 1090464, 1090709, 1092214, 1092539, 1009305, 1093123, 1093222]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load demo forecast file\n",
    "forecast_demo = pd.read_csv(\"forecast_per_customer_demo.csv\")\n",
    "\n",
    "# Show 10 unique customer IDs to test\n",
    "sample_ids = forecast_demo['customer_id'].drop_duplicates().head(10).tolist()\n",
    "print(\" Sample Customer IDs:\", sample_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af440f-db0a-4588-a9ed-e8d1c175f9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
